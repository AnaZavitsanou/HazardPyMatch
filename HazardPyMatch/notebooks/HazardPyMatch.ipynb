{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ReadMe\n",
        "This is a Chemical Inventory and Hazard Analysis Pipeline. This script prompts the\n",
        "user for Globally Harmonized System (GHS) H-codes; see here for reference:\n",
        "https://pubchem.ncbi.nlm.nih.gov/ghs . There are 2 inputs. The first is a required Chemical Inventory or list stored as an .xlsx or .csv file. This file should have \"Chemical_Inventory\" in the file name and contain 2 required column names: \"Chemical\n",
        "Name\" and \"CAS Number\". Please view the code block at the top of the pipeline. We recommend you run this code in Google Colab. Upload your chemical inventory to a Google Drive folder, making sure that the file path listed to the right of \"source_folder\" in the first code block of the pipeline code is correct. The second input is an optional folder containing PDF versions of laboratory protocols. Please store the protocols folder in the \"source_folder\" in your Google Drive ensuring that the name of the folder listed to the right of source_folder in the \"protocols_folder\" variable is correct.\n",
        "\n",
        "We set \"print_intermediate_steps\" to false by default. Change to print_intermediate_steps = True if you want to print the chemical inventory at each step for debugging.\n",
        "\n",
        "After uploading the chemical inventory, defining source_folder, protocols_folder, GHS H-codes and changing \"print_intermediate_steps\" if necessary, you can run the entire pipeline via the File menu at Runtime > Run all.\n",
        "\n",
        "The inventory file is automatically loaded from your specified source folder, with missing CAS numbers populated using PubChem API. The pipeline retrieves GHS hazard classifications (H-codes), chemical name synonyms, and searches for protocol PDFs that mention these chemicals. The extracted hazard data is visualized through bar charts, Pareto analysis and lists to identify potential chemical hazards and their protocol associations.\n",
        "\n",
        "The final outputs include processed chemical inventory lists, matched protocol hazards, and visual analytics, all saved as Excel files and PNG charts in \"source_folder\" for further analysis."
      ],
      "metadata": {
        "id": "d1GbnQ4OELqN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hghpF6RJZVxD"
      },
      "source": [
        "# Define source folder, chemical inventory and protocols folder names and locations\n",
        "\n",
        "Chemical Inventory file is in .xlsx and two required columns of data - chemical names and cas # should be named 'Chemical Name' and 'CAS Number' respectively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aeY9MamXxes"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define all person specific locations and file names\n",
        "# Define source folder\n",
        "source_folder = '/content/drive/My Drive/your source folder'\n",
        "\n",
        "# Define location of Chemical Inventory, which is an input list including 'Chemical Names' and 'CAS Numbers' column names\n",
        "inventory_files = glob.glob(os.path.join(source_folder, '*Chemical_Inventory*.xlsx'))\n",
        "\n",
        "# Define protocol folder\n",
        "protocols_folder = os.path.join(source_folder, 'your_protocols_folder')\n",
        "\n",
        "# = True if you want to print intermediate steps of chemical inventory dataframe for debugging.\n",
        "# Set at = False as default\n",
        "print_intermediate_steps = False\n",
        "\n",
        "# Prompt the user to input relevant GHS codes\n",
        "print(\"Enter relevant GHS codes separated by commas (e.g., H200,H201,H360FD):\")\n",
        "relevant_ghs_codes = input().strip().split(',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOopQCerYoPv"
      },
      "source": [
        "# Import packages and load Chemical Inventory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PX5FG8KDn3o"
      },
      "outputs": [],
      "source": [
        "# Import all necessary packages and load chemical inventory\n",
        "\n",
        "!pip install thermo\n",
        "!pip install bs4\n",
        "!pip install pdfplumber\n",
        "!pip install pubchempy\n",
        "\n",
        "#import packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from thermo.chemical import Chemical\n",
        "import pubchempy as pcp\n",
        "import tkinter\n",
        "from tkinter import *\n",
        "from tkinter import filedialog\n",
        "import pdfplumber\n",
        "import requests\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import networkx as nx\n",
        "\n",
        "df_inventory = pd.DataFrame()\n",
        "# Load .xlsx or .csv Chemical Inventory file\n",
        "try:\n",
        "    if len(inventory_files) == 0:\n",
        "        print(\"No .xlsx files found with 'Chemical Inventory' in the name. Searching for .csv files...\")\n",
        "\n",
        "        # Search for .csv files\n",
        "        inventory_files = glob.glob(os.path.join(source_folder, '*Chemical_Inventory*.csv'))\n",
        "\n",
        "        if len(inventory_files) == 0:\n",
        "            print(\"No .csv files found either. Please check the source folder.\")\n",
        "        else:\n",
        "            # Use the first .csv file that matches the pattern\n",
        "            csv_path = inventory_files[0]\n",
        "            print(f\"Loading file: {csv_path}\")\n",
        "            # Load the .csv file into a DataFrame\n",
        "            df_inventory = pd.read_csv(\n",
        "                csv_path,\n",
        "                usecols=[\"Chemical Name\", \"CAS Number\"]  # Adjust columns to match your file\n",
        "            )\n",
        "            print(\"CSV file loaded successfully.\")\n",
        "    else:\n",
        "        # Use the first .xlsx file that matches the pattern\n",
        "        excel_path = inventory_files[0]\n",
        "        print(f\"Loading file: {excel_path}\")\n",
        "        # Load the .xlsx file into a DataFrame\n",
        "        df_inventory = pd.read_excel(\n",
        "            excel_path,\n",
        "            engine='openpyxl',\n",
        "            usecols=[\"Chemical Name\", \"CAS Number\"]  # Adjust columns to match your file\n",
        "        )\n",
        "        print(\"Excel file loaded successfully.\")\n",
        "\n",
        "    # Display the first few rows of the DataFrame\n",
        "    print(df_inventory.head())\n",
        "except ValueError as ve:\n",
        "    print(f\"Error loading file: {ve}\")\n",
        "except FileNotFoundError as fnfe:\n",
        "    print(f\"File not found: {fnfe}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FybL6k-prBVf"
      },
      "source": [
        "#CAS Number filter\n",
        "Chemicals with no CAS numbers are mostly proprietary reagents.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsHz1XMjqDS6"
      },
      "outputs": [],
      "source": [
        "print(f\"....................Populating missing CAS Numbers\")\n",
        "\n",
        "def get_cas_number(chemical_name):\n",
        "    try:\n",
        "        # Construct the primary search URL for PubChem\n",
        "        search_url = f\"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/{chemical_name}/xrefs/RegistryID/JSON\"\n",
        "        response = requests.get(search_url)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            # Extract CAS Number from response content\n",
        "            response_data = response.json()\n",
        "            # Locate the CAS Number in the JSON response\n",
        "            registry_ids = response_data.get(\"InformationList\", {}).get(\"Information\", [])\n",
        "            for entry in registry_ids:\n",
        "                if \"CAS\" in entry.get(\"RegistryID\", \"\"):\n",
        "                    return entry[\"RegistryID\"]  # Return the CAS Number\n",
        "\n",
        "        # If the first URL does not return a result, try the fallback URL\n",
        "        fallback_url = f\"https://pubchem.ncbi.nlm.nih.gov/rest/pug/substance/name/{chemical_name}/xrefs/RegistryID/JSON\"\n",
        "        response = requests.get(fallback_url)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            # Extract CAS Number from the fallback response\n",
        "            response_data = response.json()\n",
        "            registry_ids = response_data.get(\"InformationList\", {}).get(\"Information\", [])\n",
        "            for entry in registry_ids:\n",
        "                if \"CAS\" in entry.get(\"RegistryID\", \"\"):\n",
        "                    return entry[\"RegistryID\"]  # Return the CAS Number\n",
        "\n",
        "        return None  # Return None if no result is found in both URLs\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error finding CAS Number for {chemical_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "for index, row in df_inventory.iterrows():\n",
        "    cas_number_individ = str(row[\"CAS Number\"]).strip()  # Convert to string and strip spaces\n",
        "\n",
        "    # Remove \"00:00:00\" or other trailing text from the CAS Number if present\n",
        "    cas_number_individ = cas_number_individ.split(\" \")[0].strip()  # Take only the first part before any space\n",
        "\n",
        "    # Define a set of invalid characters\n",
        "    invalid_characters = set(\",./?!@#$%^&*():;\\\"'\")\n",
        "\n",
        "    # Check if CAS Number contains alphabetical letters or :\n",
        "    for index, cas_number_individ in df_inventory[\"CAS Number\"].items():\n",
        "    # Convert the CAS Number to a string, handle NaN values by treating them as empty strings\n",
        "        cas_number_individ = str(cas_number_individ) if not pd.isna(cas_number_individ) else \"\"\n",
        "        # Check if there are any letters or invalid symbols in the CAS Number\n",
        "        if any(char.isalpha() or char in invalid_characters for char in cas_number_individ):\n",
        "            df_inventory.at[index, \"CAS Number\"] = \"\"  # Clear the CAS Number cell\n",
        "\n",
        "    # Ensure CAS Number follows the correct format (remove leading zero in the last segment)\n",
        "    if \"-\" in cas_number_individ:\n",
        "        parts = cas_number_individ.split(\"-\")\n",
        "        if len(parts) == 3:\n",
        "            try:\n",
        "                parts[2] = str(int(parts[2]))  # Remove leading zeros from the last segment\n",
        "                cas_number_individ = \"-\".join(parts)\n",
        "                df_inventory.at[index, \"CAS Number\"] = cas_number_individ\n",
        "            except ValueError:\n",
        "                print(f\"Error processing CAS Number: {cas_number_individ}\")\n",
        "                df_inventory.at[index, \"CAS Number\"] = \"\"  # Clear the invalid CAS Number\n",
        "                continue\n",
        "\n",
        "    # Check if the CAS Number is missing (NaN, blank, or 0)\n",
        "    if pd.isna(row[\"CAS Number\"]) or row[\"CAS Number\"] in [\"\", 0]:\n",
        "        # Try to find the CAS Number using the function\n",
        "        chemical_name = row[\"Chemical Name\"]  # Extract the chemical name\n",
        "        cas_number = get_cas_number(chemical_name)  # Fetch CAS Number\n",
        "\n",
        "        # If a CAS Number is retrieved, update the DataFrame\n",
        "        if cas_number:\n",
        "            df_inventory.at[index, \"CAS Number\"] = cas_number\n",
        "\n",
        "\n",
        "\n",
        "print(f\"....................Extracting and saving list of Chemical Names with no CAS Number\")\n",
        "\n",
        "# Initialize a new DataFrame for proprietary or other chemicals\n",
        "df_proprietaryRxs_andOther = pd.DataFrame(columns=[\"Chemical Name\", \"CAS Number\"])\n",
        "\n",
        "# Process each row to fill CAS Numbers or move to df_proprietaryRxs_andOther\n",
        "for index, row in df_inventory.iterrows():\n",
        "    if pd.isna(row[\"CAS Number\"]) or row[\"CAS Number\"] in [\"\", 0]:\n",
        "        cas_number = get_cas_number(row[\"Chemical Name\"])\n",
        "        if cas_number:\n",
        "            df_inventory.at[index, \"CAS Number\"] = cas_number\n",
        "        else:\n",
        "            # Add the row to df_proprietaryRxs_andOther\n",
        "            df_proprietaryRxs_andOther = pd.concat(\n",
        "                [df_proprietaryRxs_andOther, pd.DataFrame([row])],\n",
        "                ignore_index=True\n",
        "            )\n",
        "\n",
        "# Remove rows with NaN, blank, or 0 in CAS Number from df_inventory\n",
        "df_inventory = df_inventory[\n",
        "    ~(df_inventory[\"CAS Number\"].isnull() | (df_inventory[\"CAS Number\"] == \"\") | (df_inventory[\"CAS Number\"] == 0))\n",
        "]\n",
        "\n",
        "if print_intermediate_steps:\n",
        "  # Save df_inventory to an Excel file\n",
        "  output_filename = \"df_inventory_withCAS.xlsx\"\n",
        "  output_path = os.path.join(source_folder, output_filename)\n",
        "  df_inventory.to_excel(output_path, index=False)\n",
        "  output_filename = None\n",
        "  output_path = None\n",
        "\n",
        "# Remove rows with non-unique Chemical Names, keeping first instance\n",
        "df_proprietaryRxs_andOther[\"Chemical Name\"] = df_proprietaryRxs_andOther[\"Chemical Name\"].str.strip().str.lower()\n",
        "df_proprietaryRxs_andOther = df_proprietaryRxs_andOther.drop_duplicates(subset=\"Chemical Name\", keep=\"first\")\n",
        "df_proprietaryRxs_andOther[\"Chemical Name\"] = df_proprietaryRxs_andOther[\"Chemical Name\"].str.title()\n",
        "\n",
        "\n",
        "# Save df_proprietaryRxs_andOther to an Excel file\n",
        "output_filename = \"Chemical_List_noCAS.xlsx\"\n",
        "output_path = os.path.join(source_folder, output_filename)\n",
        "df_proprietaryRxs_andOther.to_excel(output_path, index=False)\n",
        "output_filename = None\n",
        "output_path = None\n",
        "\n",
        "df_inventory_backup = df_inventory.copy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dNl64toJ_w2"
      },
      "source": [
        "#GHS H-Code Scraper\n",
        "The code in the next two code blocks is minimally edited from github user chadr1989 at this source:\n",
        "\n",
        "https://github.com/chadr1989/GHS-codes-from-CAS-numbers/blob/main/GHS_codes_NCBI_scrape.ipynb\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHtSB65ix_HK"
      },
      "outputs": [],
      "source": [
        "#  This cell will loop over the PubChem IDs and request GHS data from NCBI.  This could have a runtime of several minutes depending on your hardware and internet connection.\n",
        "#  Chemicals will be matched to the PubChem compound database. If it is not found there try the substance database\n",
        "\n",
        "result = requests.get(f'https://pubchem.ncbi.nlm.nih.gov/ghs/#_prec','lxml')\n",
        "soup = BeautifulSoup(result.text,'lxml')\n",
        "\n",
        "gross_precautions_list = list()\n",
        "p_codes_list = list()\n",
        "precaution_statements_list = list()\n",
        "\n",
        "for i in soup.select('#pcode')[0].select('td'):\n",
        "    gross_precautions_list.append(i.text)\n",
        "for i in range(0,len(gross_precautions_list)):\n",
        "    if not re.search(r'P\\d\\d\\d',gross_precautions_list[i])==None:\n",
        "        p_codes_list.append(gross_precautions_list[i])\n",
        "for code in p_codes_list:\n",
        "    precaution_statements_list.append(gross_precautions_list[gross_precautions_list.index(code)+1])\n",
        "precaution_data_dict = {'P Codes':p_codes_list,'Precautionary Statements':precaution_statements_list}\n",
        "df_precaution = pd.DataFrame(precaution_data_dict)\n",
        "#  Note: The loaded spreadsheet must contain the CAS numbers of the chemicals in a column named \"CAS\"\n",
        "#  This cell creates new columns in the dataframe and cross-references PubChem IDs with given CAS numbers\n",
        "\n",
        "# Initialize new columns in df_inventory\n",
        "df_inventory['PubChem ID'] = np.nan\n",
        "df_inventory['GHS Codes'] = np.nan\n",
        "df_inventory['Precautionary Statements'] = np.nan\n",
        "\n",
        "# Loop through each CAS number in the DataFrame and get PubChem ID (CID) either from compound database or substance database\n",
        "for i in df_inventory['CAS Number']:\n",
        "    try:\n",
        "        # Attempt to find PubChem ID using thermo.chemical.Chemical\n",
        "        chem = Chemical(f'{i}')\n",
        "        if chem.PubChem:\n",
        "            df_inventory.loc[df_inventory['CAS Number'] == i, 'PubChem ID'] = chem.PubChem\n",
        "        else:\n",
        "            raise ValueError(\"No PubChem ID found via thermo.chemical.Chemical\")\n",
        "    except Exception:\n",
        "        try:\n",
        "            # First attempt: Use PubChem Compound API\n",
        "            compound_url = f\"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/{i}/cids/JSON\"\n",
        "\n",
        "            response = requests.get(compound_url)\n",
        "            response.raise_for_status()  # Raise an error for HTTP issues\n",
        "\n",
        "            # Parse JSON response for compound\n",
        "            data = response.json()\n",
        "\n",
        "            if 'IdentifierList' in data and 'CID' in data['IdentifierList']:\n",
        "                cid = data['IdentifierList']['CID'][0]  # Get the first CID\n",
        "                df_inventory.loc[df_inventory['CAS Number'] == i, 'PubChem ID'] = cid\n",
        "                print(f\"CID {cid} successfully added to 'PubChem ID' for CAS Number: {i}\")  # Only debugging line retained\n",
        "            else:\n",
        "                raise ValueError(\"No CID found in Compound API response\")\n",
        "\n",
        "        except Exception:\n",
        "            try:\n",
        "                # Fallback: Use PubChem Substance API\n",
        "                substance_url = f\"https://pubchem.ncbi.nlm.nih.gov/rest/pug/substance/name/{i}/cids/JSON\"\n",
        "\n",
        "                response = requests.get(substance_url)\n",
        "                response.raise_for_status()  # Raise an error for HTTP issues\n",
        "\n",
        "                # Parse JSON response for substance\n",
        "                data = response.json()\n",
        "\n",
        "                if 'InformationList' in data and 'Information' in data['InformationList']:\n",
        "                    # Extract CID from the first match in the substance response\n",
        "                    cids = [\n",
        "                        cid\n",
        "                        for info in data['InformationList']['Information']\n",
        "                        if 'CID' in info\n",
        "                        for cid in info['CID']\n",
        "                    ]\n",
        "                    if cids:\n",
        "                        df_inventory.loc[df_inventory['CAS Number'] == i, 'PubChem ID'] = cids[0]\n",
        "                        print(f\"CID {cids[0]} successfully added to 'PubChem ID' using substance database for CAS Number: {i}\")  # Only debugging line retained\n",
        "                    else:\n",
        "                        raise ValueError(\"No CID found in Substance API response\")\n",
        "                else:\n",
        "                    raise ValueError(\"No information found in Substance API response\")\n",
        "            except Exception:\n",
        "                pass  # Fail silently for any unhandled errors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnERyLjAY_kM"
      },
      "outputs": [],
      "source": [
        "for chem_id in set(df_inventory['PubChem ID'].dropna()):\n",
        "    result = requests.get(f'https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/compound/{int(chem_id)}/JSON/?response_type=display&heading=GHS%20Classification','lxml')\n",
        "    soup = BeautifulSoup(result.text,'lxml').text\n",
        "    if len(soup) > 90:\n",
        "        pattern_hits = [m.start() for m in re.finditer(r'\"H\\d\\d\\d', soup)]\n",
        "        ghs_codes_set = set()\n",
        "        for i in range(0,len(pattern_hits)):\n",
        "            phrase_start = pattern_hits[i]\n",
        "            phrase_end = soup[phrase_start+1:].find('\"')\n",
        "            ghs_codes_set.add(soup[phrase_start+1:phrase_start+5])\n",
        "        ghs_codes_list = list()\n",
        "        for c in ghs_codes_set:\n",
        "            ghs_start = soup.find(c)\n",
        "            ghs_end = soup[soup.find(c)+1:].find('\"')+1\n",
        "            ghs_codes_list.append(soup[ghs_start:ghs_start+ghs_end])\n",
        "        joined_ghs = ' --- '.join(ghs_codes_list)\n",
        "        df_inventory.loc[df_inventory['PubChem ID']==chem_id,'GHS Codes'] = joined_ghs\n",
        "        p_list = list()\n",
        "        for i in set(soup[soup.find('Precautionary Statement Codes'):].replace('and ','').replace(' ','').replace('\"\\n}','').split(',')):\n",
        "            if i in p_codes_list:\n",
        "                p_list.append(i+' '+df_precaution['Precautionary Statements'][df_precaution[df_precaution['P Codes']==i].index[0]])\n",
        "        joined_p = ' --- '.join(p_list)\n",
        "        df_inventory.loc[df_inventory['PubChem ID']==chem_id,'Precautionary Statements'] = joined_p\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "# Remove precautionary statements and show dataframe. These fail to extract from pubchem. Retain the code because the extraction fails without it.\n",
        "df_inventory = df_inventory.iloc[:, :-1]\n",
        "df_inventory.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5blXK53zmWif"
      },
      "outputs": [],
      "source": [
        "# Function to fetch GHS codes from PubChem API if this didn't work with compounds and Pubchem ID (most likely the chemical name is missing a PubchemID)\n",
        "def fetch_ghs_code(chemical_name):\n",
        "    try:\n",
        "        response = requests.get(f'https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/substance/{int(chemical_name)}/JSON/?response_type=display&heading=GHS%20Classification','lxml')\n",
        "        soup = BeautifulSoup(result.text,'lxml').text\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            # Extracting GHS Codes, placeholder logic (adjust based on actual API structure)\n",
        "            sections = data.get(\"Record\", {}).get(\"Section\", [])\n",
        "            for section in sections:\n",
        "                if \"GHS Classification\" in section.get(\"TOCHeading\", \"\"):\n",
        "                    return section.get(\"Information\", [{}])[0].get(\"StringValue\", \"No GHS Codes Found\")\n",
        "            return \"No GHS Codes Found\"\n",
        "        else:\n",
        "            return f\"Error {response.status_code}: Unable to fetch data\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "# Loop through each row and update blank GHS Codes\n",
        "for index, row in df_inventory.iterrows():\n",
        "    if not row[\"GHS Codes\"]:  # Check if GHS Codes is blank\n",
        "        chemical_name = row[\"Chemical Name\"]\n",
        "        ghs_code = fetch_ghs_code(chemical_name)\n",
        "        df_inventory.at[index, \"GHS Codes\"] = ghs_code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wn6xzQV_ZG9E"
      },
      "outputs": [],
      "source": [
        "# Save the df_inventory with GHS codes DataFrame to an Excel file\n",
        "if print_intermediate_steps:\n",
        "  output_path = None\n",
        "  output_filename = None\n",
        "  output_filename = 'df_inventory_withGHScodes.xlsx'\n",
        "  output_path = os.path.join(source_folder, output_filename)\n",
        "  df_inventory.to_excel(output_path, index=False)\n",
        "  output_path = None\n",
        "  output_filename = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myic8w03sigi"
      },
      "source": [
        "#GHS H-Code filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6obTrGyWIXm"
      },
      "outputs": [],
      "source": [
        "# Initialize DataFrames for separating relevant and other GHS codes\n",
        "relevant_ghs_df = pd.DataFrame(columns=df_inventory.columns)\n",
        "other_ghs_df = pd.DataFrame(columns=df_inventory.columns)\n",
        "\n",
        "# Regex pattern to find H### codes\n",
        "pattern = r'H\\d{3}[A-Z]*'\n",
        "\n",
        "# Loop through each row in df_inventory and check the GHS Codes column\n",
        "for index, row in df_inventory.iterrows():\n",
        "    # Extract all H### codes from the GHS Codes cell\n",
        "    ghs_codes = re.findall(pattern, str(row['GHS Codes'])) if not pd.isna(row['GHS Codes']) else []\n",
        "\n",
        "    # Check if any relevant GHS code exists in this row\n",
        "    if any(code in relevant_ghs_codes for code in ghs_codes):\n",
        "        relevant_ghs_df = pd.concat([relevant_ghs_df, row.to_frame().T], ignore_index=True)\n",
        "    else:\n",
        "        other_ghs_df = pd.concat([other_ghs_df, row.to_frame().T], ignore_index=True)\n",
        "\n",
        "# Display the first few rows of each list for confirmation\n",
        "print(\"\\nRelevant GHS Codes DataFrame:\")\n",
        "print(relevant_ghs_df.head())\n",
        "print(\"\\nIrrelevant GHS Codes DataFrame:\")\n",
        "print(other_ghs_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJRzwzAJbDU9"
      },
      "outputs": [],
      "source": [
        "# Save dataframe with all chemicals in inventory with relevant GHS codes to an Excel file\n",
        "output_filename = None\n",
        "output_path = None\n",
        "output_filename = 'All_Inventory_Chemicals_with_relevantGHScodes.xlsx'\n",
        "output_path = os.path.join(source_folder, output_filename)\n",
        "relevant_ghs_df.to_excel(output_path, index=False)\n",
        "\n",
        "if print_intermediate_steps:\n",
        "  output_filename = None\n",
        "  output_path = None\n",
        "  output_filename = 'All_Inventory_Chemicals_with_irrelevantGHScodes.xlsx'\n",
        "  output_path = os.path.join(source_folder, output_filename)\n",
        "  other_ghs_df.to_excel(output_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDkf1THeszSS"
      },
      "source": [
        "#Unique Chemical Name filter & Compiler of in-list synonyms for unique CAS Numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RdcJcmCcRKa"
      },
      "outputs": [],
      "source": [
        "#replace df_inventory (all GHS codes) with relevant_ghs_df\n",
        "df_inventory = relevant_ghs_df\n",
        "\n",
        "# Convert CAS Number column to string type to avoid type comparison issues\n",
        "df_inventory['CAS Number'] = df_inventory['CAS Number'].astype(str)\n",
        "\n",
        "# Sort the dataframe based on 'CAS Number' in chronological order\n",
        "df_sorted = df_inventory.sort_values(by='CAS Number')\n",
        "\n",
        "# Create a new dataframe to store unique CAS Numbers\n",
        "unique_cas_nums = pd.DataFrame(columns=df_inventory.columns)\n",
        "\n",
        "# List to collect data for the new chemical name columns\n",
        "new_columns_data = []\n",
        "\n",
        "# Loop through the sorted dataframe to extract first entry for each unique CAS Number\n",
        "for cas_number, group in df_sorted.groupby('CAS Number'):\n",
        "    # Get the first row for the unique CAS Number\n",
        "    first_entry = group.iloc[0].to_frame().T\n",
        "\n",
        "    # Use pd.concat to append the first entry to unique_cas_nums\n",
        "    unique_cas_nums = pd.concat([unique_cas_nums, first_entry], ignore_index=True)\n",
        "\n",
        "    # Create a temporary list of Chemical Names matching the unique CAS Number\n",
        "    chemical_names = group['Chemical Name'].tolist()\n",
        "\n",
        "    # Remove duplicates from the chemical names list\n",
        "    unique_chemical_names = list(dict.fromkeys(chemical_names))\n",
        "\n",
        "    # Add the unique chemical names as a list (will be used to create new columns)\n",
        "    new_columns_data.append(unique_chemical_names)\n",
        "\n",
        "# Ensure new_columns_data is not empty before applying max()\n",
        "if not new_columns_data or all(len(names) == 0 for names in new_columns_data):\n",
        "    print(\"You have no chemicals in this inventory with relevant GHS codes!\")\n",
        "else:\n",
        "    # Create a new dataframe for the chemical names, handling varying numbers of chemical names per CAS Number\n",
        "    max_names = max([len(names) for names in new_columns_data])\n",
        "    chemical_names_df = pd.DataFrame([\n",
        "        names + [None] * (max_names - len(names)) for names in new_columns_data\n",
        "    ], columns=[f'inventory synonym {i+1}' for i in range(max_names)])\n",
        "\n",
        "    # Concatenate the chemical names dataframe with unique_cas_nums\n",
        "    unique_cas_nums = pd.concat([unique_cas_nums.reset_index(drop=True), chemical_names_df.reset_index(drop=True)], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YakO9WC9eJD4"
      },
      "outputs": [],
      "source": [
        "# Save the df_inventory with unique GHS codes DataFrame to an Excel file\n",
        "if print_intermediate_steps:\n",
        "  output_filename = None\n",
        "  output_path = None\n",
        "  output_filename = 'df_inventory_relevantGHScodes_uniquecodes_inlistsyns.xlsx'\n",
        "  output_path = os.path.join(source_folder, output_filename)\n",
        "  unique_cas_nums.to_excel(output_path, index=False)\n",
        "  output_filename = None\n",
        "  output_path = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd9UTt0okBnp"
      },
      "source": [
        "#PubChem Synonym requests using CIDs utilizing PUG REST service\n",
        "Try hazards as compounds (CIDs) first, then substances (SIDs).\n",
        "<br> Resource websites:\n",
        "- https://pubchem.ncbi.nlm.nih.gov/docs/pug-rest-tutorial#section=How-PUG-REST-Works <br>\n",
        "- https://pubchem.ncbi.nlm.nih.gov/docs/pug-rest#section=Synonyms\n",
        "\n",
        "Example url for request: https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/vioxx/synonyms/TXT\n",
        "<br> Only change CID (in example: vioxx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01W-hvlCkoQL"
      },
      "outputs": [],
      "source": [
        "# Replace df_inventory with unique_cas_nums\n",
        "df_inventory = unique_cas_nums\n",
        "\n",
        "# Define master list\n",
        "master_list = df_inventory['CAS Number'].astype(str).str.strip().tolist()\n",
        "\n",
        "def search_hazard_synonyms_and_save_to_excel(master_list, input_df):\n",
        "    \"\"\"\n",
        "    Search for synonyms of CAS Numbers using the PubChem API and return the updated DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    master_list (list): List of CAS Numbers to search for.\n",
        "    input_df (pd.DataFrame): The original DataFrame containing CAS Numbers and other information.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: The merged DataFrame with synonyms.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Ensure the CAS Numbers in master_list are strings and stripped of spaces\n",
        "    master_list = [str(cas_number).strip() for cas_number in master_list]\n",
        "\n",
        "    for cas_number in master_list:\n",
        "        print(f'Processing CAS Number: {cas_number}')  # Debugging step to verify CAS numbers\n",
        "\n",
        "        try:\n",
        "            # Create the URL for retrieving compound synonyms based on CAS Number\n",
        "            url = f'https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/{cas_number}/synonyms/TXT'\n",
        "\n",
        "            # Make an HTTP GET request to the PubChem API\n",
        "            response = requests.get(url)\n",
        "\n",
        "            # Check if the request was successful\n",
        "            if response.status_code == 200:\n",
        "                synonyms_text = response.text.strip().split(\"\\n\")\n",
        "                #print(f'CAS Number: {cas_number}')\n",
        "                #print(f'Synonyms: {synonyms_text}')\n",
        "                # Append the CAS number and all synonyms to the results list\n",
        "                results.append([cas_number] + synonyms_text)\n",
        "            elif response.status_code == 404:\n",
        "                substance_url = f'https://pubchem.ncbi.nlm.nih.gov/rest/pug/substance/name/{cas_number}/synonyms/TXT'\n",
        "                substance_response = requests.get(substance_url)\n",
        "\n",
        "                if substance_response.status_code == 200:\n",
        "                    synonyms_text = substance_response.text.strip().split(\"\\n\")\n",
        "                    results.append([cas_number] + synonyms_text)\n",
        "                else:\n",
        "                    print(f'Substance not found or another error: {substance_response.status_code}')\n",
        "                    results.append([cas_number, f'Substance Error: {substance_response.status_code}'])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f'Error processing CAS Number: {cas_number}')\n",
        "            print(f'Error message: {str(e)}')\n",
        "            results.append([cas_number, f'Error: {str(e)}'])\n",
        "\n",
        "    # Convert the results to a DataFrame (expand columns dynamically based on the max number of synonyms)\n",
        "    df_synonyms = pd.DataFrame(results)\n",
        "\n",
        "    # Assign column names dynamically: first column as 'CAS Number' and rest as 'Synonym 1', 'Synonym 2', etc.\n",
        "    max_synonyms = df_synonyms.shape[1] - 1  # The first column is 'CAS Number', the rest are synonyms\n",
        "    df_synonyms.columns = ['CAS Number'] + [f'Synonym {i}' for i in range(1, max_synonyms + 1)]\n",
        "\n",
        "    # Merge the synonyms DataFrame back with the original DataFrame\n",
        "    output_df = pd.merge(input_df, df_synonyms, on=\"CAS Number\", how=\"left\")\n",
        "\n",
        "    # Return the DataFrame for further processing or saving\n",
        "    return output_df\n",
        "\n",
        "\n",
        "# Call the function and get the output DataFrame\n",
        "output_df = search_hazard_synonyms_and_save_to_excel(master_list, df_inventory)\n",
        "\n",
        "# Print the header for debugging\n",
        "print(\"Output DataFrame Header:\")\n",
        "print(output_df.columns)\n",
        "\n",
        "# Save the output DataFrame to an Excel file\n",
        "if print_intermediate_steps:\n",
        "  output_path = None\n",
        "  output_filename = None\n",
        "  output_filename = 'df_inventory_relevantGHScodes_uniquecodes_inlistsyns_ncbisyns.xlsx'\n",
        "  output_path = os.path.join(source_folder, output_filename)\n",
        "  output_df.to_excel(output_path, index=False)\n",
        "  print(f\"Excel file saved to: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUJoC_O8texT"
      },
      "outputs": [],
      "source": [
        "# Replace df_inventory with output_df\n",
        "df_inventory = output_df\n",
        "\n",
        "print('Removing duplicate synonyms')\n",
        "# Iterate through each row and dynamically determine the range from column E (index 4) to the last column\n",
        "for index, row in df_inventory.iterrows():\n",
        "    # Calculate the shape (number of elements) of the row before changes\n",
        "    before_shape = len([val for val in row[4:].values if pd.notna(val)])\n",
        "\n",
        "    for col in df_inventory.columns[4:df_inventory.shape[1]]:  # Dynamically select up to the last column\n",
        "        if pd.notna(row[col]):  # Only process non-NaN cells\n",
        "            # Split the cell content into a list of values, remove duplicates while maintaining order, and rejoin\n",
        "            unique_values = list(dict.fromkeys(row[col].split(\";\")))\n",
        "            df_inventory.at[index, col] = \";\".join(unique_values)\n",
        "\n",
        "    # Calculate the shape (number of elements) of the row after changes\n",
        "    after_shape = len([val for val in row[4:].values if pd.notna(val)])\n",
        "\n",
        "dims=df_inventory.shape\n",
        "print(dims)\n",
        "\n",
        "output_path = None\n",
        "output_filename = None\n",
        "output_filename = 'df_inventory_relevantGHScodes_uniquecodes_inlistsyns_ncbisyns.xlsx'\n",
        "output_path = os.path.join(source_folder, output_filename)\n",
        "output_df.to_excel(output_path, index=False)\n",
        "print(f\"Excel file saved to: {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPwWIManXM0S"
      },
      "source": [
        "# Pdfplumber dependent protocol search, match and record"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qN8yTsVBTnMI"
      },
      "outputs": [],
      "source": [
        "# Populate protocols with the names of PDF files\n",
        "protocols = []\n",
        "for filename in os.listdir(protocols_folder):\n",
        "    if filename.endswith('.pdf'):\n",
        "        protocols.append(filename[:-4])\n",
        "print(protocols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po4X44Abdb8r"
      },
      "outputs": [],
      "source": [
        "# This is the protocol pdf matching code. Toward completeness, we include all in list and PubChem extracted synonyms for each CAS number\n",
        "# This results in occasional false positives particuarly when PubChem synonym list contains commonly used words like 'Clean' or 'not have'\n",
        "# The code outputs a hazards detail list so you can view the synonym found in the protocol pdf file and ignore the false positives.\n",
        "\n",
        "# Initialize the master list and hazards list\n",
        "master_list = []\n",
        "hazards = []\n",
        "\n",
        "# Populate the master list with CAS Numbers, synonyms, PubChem ID, and GHS Codes\n",
        "for index, row in df_inventory.iterrows():\n",
        "    cas_number = row['CAS Number']\n",
        "    pubchem_id = row['PubChem ID']\n",
        "    ghs_codes = row['GHS Codes']\n",
        "    synonyms = row[4:]  # Get all synonyms starting from the 5th column\n",
        "\n",
        "    for synonym in synonyms:\n",
        "        if pd.notna(synonym):  # Ensure synonym is not NaN\n",
        "            # Append a tuple with CAS number, synonym, PubChem ID, and GHS Codes\n",
        "            master_list.append((cas_number, synonym.strip(), pubchem_id, ghs_codes))\n",
        "\n",
        "# Convert master_list to a DataFrame for easier handling\n",
        "master_list_df = pd.DataFrame(master_list, columns=['CAS Number', 'Synonym', 'PubChem ID', 'GHS Codes'])\n",
        "master_list_df.columns = master_list_df.columns.str.replace(' ', '_')\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "print(master_list_df)\n",
        "master_list_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kygij4qKUaJh"
      },
      "outputs": [],
      "source": [
        "# This is the code block where the pdf matching takes place. Code execution will take at least 2 minutes to run - with extensive lists it can take up to 3 hours to run.\n",
        "# Initialize the hazards list and the matched details list\n",
        "hazards = []\n",
        "matched_details = []\n",
        "\n",
        "# Iterate through the files in the protocols folder\n",
        "for filename in os.listdir(protocols_folder):\n",
        "    print(f\"Processing protocol: {filename}\")  # Current file being processed\n",
        "    if filename.endswith('.pdf') and filename != \"Hazards In Protocols.txt\":\n",
        "        extracted_text = ''  # Initialize extracted text for the current PDF\n",
        "        try:\n",
        "            with pdfplumber.open(os.path.join(protocols_folder, filename)) as pdf:\n",
        "                for page_num, page in enumerate(pdf.pages):\n",
        "                    page_text = page.extract_text() or ''  # Extract text from each page\n",
        "                    extracted_text += page_text  # Append the page's text\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing protocol {filename}: {e}\")  # Handle PDF processing errors\n",
        "            continue\n",
        "\n",
        "        # Search for matches in the extracted text across all synonyms\n",
        "        matched_cas_numbers = []\n",
        "        matched_pubchem_ids = []\n",
        "        matched_ghs_codes = []\n",
        "\n",
        "        for row in master_list_df.itertuples(index=True):\n",
        "            cas_number = row.CAS_Number\n",
        "            synonym = row.Synonym\n",
        "            pubchem_id = row.PubChem_ID\n",
        "            ghs_codes = row.GHS_Codes\n",
        "\n",
        "            # Perform case-sensitive whole-word matching using regex\n",
        "            pattern = fr'\\b{re.escape(synonym)}\\b'\n",
        "            if re.search(pattern, extracted_text):  # Check for whole-word match\n",
        "                matched_cas_numbers.append(cas_number)\n",
        "                matched_pubchem_ids.append(pubchem_id)\n",
        "                matched_ghs_codes.append(ghs_codes)\n",
        "\n",
        "                # Append details for the new DataFrame\n",
        "                matched_details.append({\n",
        "                    'Protocol': filename,\n",
        "                    'Synonym': synonym,\n",
        "                    'CAS Number': cas_number,\n",
        "                    'PubChem_ID': pubchem_id,\n",
        "                    'GHS_Codes': ghs_codes\n",
        "                })\n",
        "        print(matched_cas_numbers)\n",
        "        # Remove duplicate CAS numbers while keeping order\n",
        "        unique_cas_set = set()\n",
        "        unique_details = []\n",
        "        unique_matched_cas_numbers = []\n",
        "        unique_matched_pubchem_ids = []\n",
        "        unique_matched_ghs_codes = []\n",
        "\n",
        "        for i, cas in enumerate(matched_cas_numbers):\n",
        "            if cas not in unique_cas_set:\n",
        "                unique_cas_set.add(cas)\n",
        "                unique_matched_cas_numbers.append(cas)\n",
        "                unique_matched_pubchem_ids.append(matched_pubchem_ids[i])\n",
        "                unique_matched_ghs_codes.append(matched_ghs_codes[i])\n",
        "                unique_details.append(matched_details[i])\n",
        "\n",
        "        # Replace the original lists with the unique versions\n",
        "        matched_cas_numbers = unique_matched_cas_numbers\n",
        "        matched_pubchem_ids = unique_matched_pubchem_ids\n",
        "        matched_ghs_codes = unique_matched_ghs_codes\n",
        "        matched_details = unique_details\n",
        "\n",
        "        print(matched_cas_numbers)\n",
        "\n",
        "        # Prepare hazard entry for each protocol\n",
        "        list_name = filename.replace('.pdf', '')\n",
        "        parts = list_name.split('_', 1)  # Split at the first underscore\n",
        "        protocol = parts[0]\n",
        "        source = parts[1] if len(parts) > 1 else \"\"\n",
        "\n",
        "        if not matched_cas_numbers:\n",
        "            matched_hazards_str = \"N/A\"\n",
        "            print(f\"No matches found for {filename}\")\n",
        "        else:\n",
        "            matched_hazards_str = ', '.join(matched_cas_numbers)\n",
        "            print(f\"Matched CAS numbers for {filename}: {matched_hazards_str}\")\n",
        "\n",
        "        # Append protocol, source, and matched CAS numbers to hazards list\n",
        "        hazards.append([protocol, source, matched_hazards_str])\n",
        "        print(f\"Added to hazards list: {protocol}, {source}, {matched_hazards_str}\")\n",
        "\n",
        "# Convert hazards list to a DataFrame\n",
        "df_hazards = pd.DataFrame(hazards, columns=['Protocol', 'Source', 'Hazards'])\n",
        "\n",
        "# Convert matched details list to a DataFrame\n",
        "df_matched_details = pd.DataFrame(matched_details, columns=['Protocol', 'Specific word pulled from pdf', 'Matched CAS Number', 'Matched PubChem_ID', 'Matched GHS_Codes'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoZm3ZXsU0h1"
      },
      "outputs": [],
      "source": [
        "# Save the df_inventory with unique GHS codes DataFrame to an Excel file\n",
        "output_filename = None\n",
        "output_path = None\n",
        "output_filename = 'hazards_in_protocols.xlsx'\n",
        "output_path = os.path.join(source_folder, output_filename)\n",
        "df_hazards.to_excel(output_path, index=False)\n",
        "output_filename = None\n",
        "output_path = None\n",
        "\n",
        "# Save the df_inventory with unique GHS codes DataFrame to an Excel file\n",
        "output_filename = None\n",
        "output_path = None\n",
        "output_filename = 'protocol_matched_hazard_details.xlsx'\n",
        "output_path = os.path.join(source_folder, output_filename)\n",
        "df_matched_details.to_excel(output_path, index=False)\n",
        "output_filename = None\n",
        "output_path = None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data visualizations"
      ],
      "metadata": {
        "id": "28nmCGwoDk47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=df_hazards\n",
        "\n",
        "# Count the number of hazards per protocol\n",
        "df[\"Hazard Count\"] = df[\"Hazards\"].apply(lambda x: len(str(x).split(\", \")) if pd.notna(x) else 0)\n",
        "\n",
        "# Figure size\n",
        "fig_size = (12, 6)  # 100%\n",
        "font = {'family': 'DejaVu Sans', 'size': 14}\n",
        "\n",
        "# Get top 10 protocols by hazard count\n",
        "top_protocols = df.nlargest(10, \"Hazard Count\")[[\"Protocol\", \"Hazard Count\"]]\n",
        "print(\"Top 10 protocols with highest number of hazards:\")\n",
        "print(top_protocols)\n",
        "\n",
        "# Plot bar chart showing the number of hazards per protocol\n",
        "fig, ax = plt.subplots(figsize=fig_size)\n",
        "ax.bar(df[\"Protocol\"], df[\"Hazard Count\"], color=\"blue\")\n",
        "ax.set_xlabel(\"Protocol\", fontdict=font)\n",
        "ax.set_ylabel(\"Number of Hazards\", fontdict=font)\n",
        "# Display protocol names on x-axis\n",
        "ax.set_xticks(range(len(df[\"Protocol\"])))\n",
        "ax.set_xticklabels(df[\"Protocol\"], rotation=45, ha=\"right\", fontsize=8)\n",
        "ax.set_title(\"Number of Hazards per Protocol\", fontdict=font)\n",
        "\n",
        "plt.show()\n",
        "# Save bar chart to source folder\n",
        "bar_chart_path = os.path.join(source_folder, \"hazards_per_protocol.png\")\n",
        "fig.savefig(bar_chart_path)\n",
        "\n",
        "# Extract and count CAS numbers\n",
        "cas_list = df[\"Hazards\"].dropna().str.split(\", \").explode()\n",
        "cas_counts = Counter(cas_list)\n",
        "\n",
        "# Convert to DataFrame and rank by frequency\n",
        "cas_df = pd.DataFrame(cas_counts.items(), columns=[\"CAS Number\", \"Frequency\"]).sort_values(by=\"Frequency\", ascending=False)\n",
        "\n",
        "# Calculate cumulative percentage\n",
        "cas_df[\"Cumulative Frequency\"] = cas_df[\"Frequency\"].cumsum()\n",
        "cas_df[\"Cumulative Percentage\"] = (cas_df[\"Cumulative Frequency\"] / cas_df[\"Frequency\"].sum()) * 100\n",
        "\n",
        "# Determine Pareto threshold\n",
        "threshold = 80\n",
        "pareto_cutoff = cas_df[cas_df[\"Cumulative Percentage\"] <= threshold]\n",
        "\n",
        "# Plot Pareto chart\n",
        "fig, ax1 = plt.subplots(figsize=fig_size)\n",
        "\n",
        "# Bar plot for frequency\n",
        "ax1.bar(cas_df[\"CAS Number\"], cas_df[\"Frequency\"], color=\"C0\")\n",
        "ax1.set_xlabel(\"Hazard\", fontdict=font)\n",
        "ax1.set_ylabel(\"Frequency\", fontdict=font, color=\"C0\")\n",
        "ax1.tick_params(axis=\"y\", labelcolor=\"C0\")\n",
        "#x.set_xticklabels(df[\"CAS Number\"], rotation=45, ha=\"right\", fontsize=8)\n",
        "\n",
        "# Line plot for cumulative percentage\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(cas_df[\"CAS Number\"], cas_df[\"Cumulative Percentage\"], color=\"C1\", marker=\"o\", linestyle=\"-\")\n",
        "ax2.axhline(y=threshold, color=\"gray\", linestyle=\"--\")\n",
        "ax2.set_ylabel(\"Cumulative Percentage\", fontdict=font, color=\"C1\")\n",
        "ax2.tick_params(axis=\"y\", labelcolor=\"C1\")\n",
        "\n",
        "plt.title(\"Pareto Analysis of Hazard Frequency\", fontdict=font)\n",
        "plt.show()\n",
        "\n",
        "print(\"CAS Numbers over 80% Pareto Threshold:\")\n",
        "print(pareto_cutoff[\"CAS Number\"].tolist())\n",
        "print(\"CAS Numbers all:\")\n",
        "print(cas_df[\"CAS Number\"].tolist())\n",
        "\n",
        "pareto_chart_path = os.path.join(source_folder, \"pareto_analysis.png\")\n",
        "fig.savefig(pareto_chart_path)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "Is7Qf2BlDX3u"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "28nmCGwoDk47"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}